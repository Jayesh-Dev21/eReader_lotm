# ðŸ“˜ Condensed Cloudflare Scraper Prompt

```xml
<task>
Python scraper for ranobes.top with Cloudflare bypass. Works for any book by book_id.
</task>

<requirements>
BYPASS: cloudscraper + selenium fallback (undetected-chromedriver)
URLS: 
- Chapters list: /chapters/{book_id}/page/{N}/
- Extract book_id from novel URL or CLI arg
RESPECT: robots.txt (avoid /engine/, /tags/, /publisher/)
RATE LIMIT: 2-5s random delay between requests
RESUME: checkpoint.json after each page
FORMATS: JSON, SQLite (Room-compatible), TXT

CLI:
python scraper.py --book-id 133485 --format sqlite --output lotm.db
python scraper.py --url <NOVEL_URL> --format json
python scraper.py --resume checkpoint.json

STRUCTURE:
/scripts/
â”œâ”€â”€ scraper.py              # Main CLI (argparse)
â”œâ”€â”€ config.yaml             # Selectors for ranobes.top
â”œâ”€â”€ requirements.txt        # cloudscraper, selenium, undetected-chromedriver, bs4, tqdm, pyyaml
â”œâ”€â”€ README.md
â””â”€â”€ utils/
    â”œâ”€â”€ cloudflare_bypass.py    # Cloudscraper + Selenium fallback
    â”œâ”€â”€ parser.py               # Extract chapter links + content
    â”œâ”€â”€ cleaner.py              # Remove ads/scripts
    â””â”€â”€ formatter.py            # Export JSON/SQLite/TXT

CONFIG.YAML:
```yaml
ranobes.top:
  chapter_list: "https://ranobes.top/chapters/{book_id}/page/{page}/"
  selectors:
    chapter_links: "article.poster a.poster-title"
    chapter_title: "h1.chapter-title"
    chapter_content: "div.text-content"
    pagination: "div.pagination a:last-child"
  rate_limit: [2, 5]
```

SQLITE SCHEMA:
CREATE TABLE chapters (
  id INTEGER PRIMARY KEY,
  book_id TEXT,
  title TEXT,
  content TEXT,
  order_index INTEGER
);

CLOUDFLARE BYPASS:
1. Try cloudscraper.create_scraper()
2. If fails â†’ use selenium with undetected-chromedriver
3. Random User-Agent rotation
4. Retry 3x with exponential backoff

WORKFLOW:
1. GET /chapters/{book_id}/ â†’ detect total pages
2. Loop pages â†’ extract chapter links
3. For each chapter â†’ GET content, clean HTML, save
4. Checkpoint after each page, resume on failure
</requirements>

<output>
Generate: scraper.py, config.yaml, utils/*.py, requirements.txt, README.md
- Complete code, error handling, progress bars (tqdm)
- Checkpoint system, Room-compatible SQLite
- Works for any ranobes.top book_id
</output>

<execute>
Generate all files now.
</execute>
```

---

**If still too long, use this 2-step approach:**

### Step 1:
```
Create Python scraper for ranobes.top: book_id input, Cloudflare bypass (cloudscraper + selenium), scrape /chapters/{book_id}/page/{N}/, export to SQLite/JSON/TXT, checkpoint resume, 2-5s delays. Structure: scraper.py + utils/ (cloudflare_bypass, parser, formatter) + config.yaml.
```

### Step 2:
```
Add CLI (argparse), Room-compatible schema, progress bars (tqdm), error handling with retries, README with usage examples. Clean HTML (remove ads/scripts). Works for any book_id.
```